{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective of this Notebook\n",
    "This notebook uses the dataset provided by kaggle for a competition called : Don't Overfit! II. \n",
    "\n",
    "Kagle provides a test set and a very small training set, which will encourage our model to overfit.\n",
    "\n",
    "So we have to avoid overfitting and propose a model which can generalize to new examples coming from the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration\n",
    "\n",
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.067</td>\n",
       "      <td>-1.114</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>0.376</td>\n",
       "      <td>1.090</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>0.460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.958</td>\n",
       "      <td>-0.824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>0.271</td>\n",
       "      <td>1.716</td>\n",
       "      <td>1.096</td>\n",
       "      <td>1.731</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>1.904</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.765</td>\n",
       "      <td>-0.735</td>\n",
       "      <td>-1.158</td>\n",
       "      <td>2.554</td>\n",
       "      <td>0.856</td>\n",
       "      <td>-1.506</td>\n",
       "      <td>0.462</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-1.932</td>\n",
       "      <td>-0.343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>1.390</td>\n",
       "      <td>-0.732</td>\n",
       "      <td>-1.065</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-1.450</td>\n",
       "      <td>0.317</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.311</td>\n",
       "      <td>0.799</td>\n",
       "      <td>-1.001</td>\n",
       "      <td>1.544</td>\n",
       "      <td>0.575</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.646</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.916</td>\n",
       "      <td>-1.343</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.636</td>\n",
       "      <td>1.127</td>\n",
       "      <td>0.189</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.370</td>\n",
       "      <td>1.093</td>\n",
       "      <td>0.596</td>\n",
       "      <td>-0.589</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-1.081</td>\n",
       "      <td>0.805</td>\n",
       "      <td>3.401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.811</td>\n",
       "      <td>-1.509</td>\n",
       "      <td>0.522</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.718</td>\n",
       "      <td>-1.017</td>\n",
       "      <td>1.249</td>\n",
       "      <td>-0.596</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>1.751</td>\n",
       "      <td>1.442</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>-0.643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 302 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target      0      1      2      3      4      5      6      7  ...  \\\n",
       "0   0     1.0 -1.067 -1.114 -0.616  0.376  1.090  0.467 -0.422  0.460  ...   \n",
       "1   1     0.0 -0.831  0.271  1.716  1.096  1.731 -0.197  1.904 -0.265  ...   \n",
       "2   2     0.0  0.099  1.390 -0.732 -1.065  0.005 -0.081 -1.450  0.317  ...   \n",
       "3   3     1.0 -0.989 -0.916 -1.343  0.145  0.543  0.636  1.127  0.189  ...   \n",
       "4   4     0.0  0.811 -1.509  0.522 -0.360 -0.220 -0.959  0.334 -0.566  ...   \n",
       "\n",
       "     290    291    292    293    294    295    296    297    298    299  \n",
       "0  0.220 -0.339  0.254 -0.179  0.352  0.125  0.347  0.436  0.958 -0.824  \n",
       "1 -0.765 -0.735 -1.158  2.554  0.856 -1.506  0.462 -0.029 -1.932 -0.343  \n",
       "2 -1.311  0.799 -1.001  1.544  0.575 -0.309 -0.339 -0.148 -0.646  0.725  \n",
       "3 -1.370  1.093  0.596 -0.589 -0.649 -0.163 -0.958 -1.081  0.805  3.401  \n",
       "4 -0.178  0.718 -1.017  1.249 -0.596 -0.445  1.751  1.442 -0.393 -0.643  \n",
       "\n",
       "[5 rows x 302 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = pd.read_csv('train.csv')\n",
    "test_set = pd.read_csv('test.csv')\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{dtype('int64'), dtype('float64')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(training_set.dtypes.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have numerical features, we can perform regression (linear or logistic)\n",
    "\n",
    "\n",
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The False boolean tells us that we have no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target classes repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    183\n",
      "1.0     67\n",
      "Name: target, dtype: int64 \n",
      "\n",
      "Percentage of class 0: 73.2 %\n",
      "Percentage of class 1: 26.8 %\n"
     ]
    }
   ],
   "source": [
    "print(training_set['target'].value_counts(),'\\n')\n",
    "\n",
    "class0_total = training_set['target'].value_counts()[0]\n",
    "class1_total = training_set['target'].value_counts()[1]\n",
    "total =  class1_total + class0_total\n",
    "print('Percentage of class 0:',(class0_total / total)*100, \"%\")\n",
    "print('Percentage of class 1:',(class1_total / total)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the class 0 is much more present than the class 1, we are in an imbalanced case. \n",
    "Thus, later, when we'll evaluate our model, we should use an appropriate metric of evaluation for skewed classes.\n",
    "We could use **precision**, **recall** and finally compute the **F1 score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing\n",
    "### Feature scaling\n",
    "<a id='scaling-data'></a>\n",
    "\n",
    "Our linear model uses Gradient Descent to minimize a cost function.\n",
    "We can speed up gradient descent by having each of our input values in roughly the same range.\n",
    "Two techniques to help with this are feature scaling and mean normalization. \n",
    "\n",
    "* Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. \n",
    "\n",
    "* Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. \n",
    "\n",
    "-> To implement both of these techniques, we can adjust our input values as shown in this formula:\n",
    "$Z = \\frac{x - \\mu}{\\sigma}$ where x is the feature value, $\\mu$ is the average of all the values for the feature concerned and $\\sigma$ is the standard deviation (or the range value : max-min)\n",
    "\n",
    "\n",
    "Here the features are transformed so that they have the properties of a standard normal distribution  with mean = 0 and standard deviation = 1.\n",
    "This will speed up calcultations of algorithms using Gradient Descent and measures of distance (like Euclidean distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.181</td>\n",
       "      <td>-3.041</td>\n",
       "      <td>-2.967</td>\n",
       "      <td>-2.898</td>\n",
       "      <td>-2.837</td>\n",
       "      <td>-3.831</td>\n",
       "      <td>-2.873</td>\n",
       "      <td>-2.489</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.824</td>\n",
       "      <td>-2.971</td>\n",
       "      <td>-3.592</td>\n",
       "      <td>-3.071</td>\n",
       "      <td>-2.621</td>\n",
       "      <td>-3.013</td>\n",
       "      <td>-3.275</td>\n",
       "      <td>-2.665</td>\n",
       "      <td>-3.006</td>\n",
       "      <td>-2.471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>249</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.347</td>\n",
       "      <td>3.138</td>\n",
       "      <td>2.609</td>\n",
       "      <td>2.590</td>\n",
       "      <td>2.413</td>\n",
       "      <td>2.687</td>\n",
       "      <td>2.793</td>\n",
       "      <td>3.766</td>\n",
       "      <td>...</td>\n",
       "      <td>2.773</td>\n",
       "      <td>2.701</td>\n",
       "      <td>3.193</td>\n",
       "      <td>4.280</td>\n",
       "      <td>2.716</td>\n",
       "      <td>3.074</td>\n",
       "      <td>2.626</td>\n",
       "      <td>2.388</td>\n",
       "      <td>2.730</td>\n",
       "      <td>3.401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 302 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  target      0      1      2      3      4      5      6      7  ...  \\\n",
       "min    0     0.0 -3.181 -3.041 -2.967 -2.898 -2.837 -3.831 -2.873 -2.489  ...   \n",
       "max  249     1.0  2.347  3.138  2.609  2.590  2.413  2.687  2.793  3.766  ...   \n",
       "\n",
       "       290    291    292    293    294    295    296    297    298    299  \n",
       "min -2.824 -2.971 -3.592 -3.071 -2.621 -3.013 -3.275 -2.665 -3.006 -2.471  \n",
       "max  2.773  2.701  3.193  4.280  2.716  3.074  2.626  2.388  2.730  3.401  \n",
       "\n",
       "[2 rows x 302 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.agg([min, max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Features column names without the target 'target' \n",
    "features_columns_names = training_set.columns[:-1]\n",
    "scaled_data = scaler.fit_transform(training_set[features_columns_names])\n",
    "\n",
    "#Replacing features old values with their scaled values\n",
    "training_set.loc[:, features_columns_names] = scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression\n",
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "training_targets = training_set['target']\n",
    "training_predictors = training_set.drop(columns='target')\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(training_predictors, training_targets, test_size=0.30, random_state=42, shuffle=True, \n",
    "                                                    stratify=training_targets)\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_val =y_val.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression_model =LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, validation and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "#To split the training set in 50 parts of same size\n",
    "instance_sizes = np.linspace(0.05, 1.0, 50)\n",
    "train_sizes = [int(i*X_train.shape[0]) for i in instance_sizes]\n",
    "\n",
    "logloss_training_values = []\n",
    "logloss_validation_values = []\n",
    "\n",
    "for i in range(len(instance_sizes)):\n",
    "    logistic_model = logistic_regression_model\n",
    "        \n",
    "    #Training part\n",
    "    training_predictors = X_train.iloc[:train_sizes[i]]\n",
    "    training_outputs = y_train.iloc[:train_sizes[i]]\n",
    "    training_outputs = training_outputs.values.astype(int)\n",
    "    \n",
    "    logistic_model = logistic_model.fit(training_predictors , training_outputs)\n",
    "    training_predictions = logistic_model.predict(training_predictors)\n",
    "    logloss_training_values.append(log_loss(training_outputs,training_predictions))\n",
    "        \n",
    "        \n",
    "    #Test on validation set part\n",
    "    val_predictions = logistic_model.predict(X_val)\n",
    "    val_outputs = y_val.values\n",
    "    logloss_validation_values.append(log_loss(val_outputs, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "plt.plot(train_sizes,logloss_training_values, color=\"green\", label=\"Training set loss\")\n",
    "plt.plot(train_sizes,logloss_validation_values, color=\"red\",  label=\"Validation set loss\")\n",
    "\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Logistic regression learning Curve\")\n",
    "plt.xlabel(\"Training set size\"), plt.ylabel(\"Error value\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1-score: 1.0\n",
      "Validation F1-score: 0.41379310344827586\n"
     ]
    }
   ],
   "source": [
    "print(\"Training F1-score:\", f1_score(training_outputs, training_predictions))\n",
    "print(\"Validation F1-score:\", f1_score(val_outputs, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Interpretation\n",
    "\n",
    "### Learning curve interpretation\n",
    "Plotting learning curves could be a good way to check if our model is doing good.\n",
    "The loss value is very close to 0 for the training set compared to the loss for the validation set.\n",
    "\n",
    "### F1 scores interpretation\n",
    "Given that the target classes are skewed (as seen on the 'Target classes repartition' part) , we'll use the F1 score , an appropriate metric when target classes proportions are imbalanced.\n",
    "\n",
    "The relative contribution of precision and recall to the F1 score are equal. \n",
    "\n",
    "The formula for the F1 score is:\n",
    "$$F1 = 2 * \\frac{(precision * recall)}{(precision + recall)}$$\n",
    "\n",
    "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its **best value at 1** and **worst score at 0**.\n",
    "\n",
    "\n",
    "**The F1-score reaches its best value of 1.0 for the training set but reaches 0.41 for the validation set, which is much lower.**\n",
    "\n",
    "### Conclusion of the interpretations\n",
    "We can see that our model has a high variance of performance between the training set and the validation set.\n",
    "**Our model is overfitting the training set and can't generalize to new examples coming from the validation set.**\n",
    "\n",
    "### What can we do to reduce overfitting ?\n",
    "To prevent or reduce overfitting there are several options for us:\n",
    "* Use regularization techniques\n",
    "* Feature selection : choose a smaller subset of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "### Pearson correlation between features\n",
    "Pearson’s correlation coefficient is a statistical test which measures the statistical relationship, or association, between two continuous variables.  It is known as the best method of measuring the association between variables of interest because it is based on the method of covariance.  It gives information about the magnitude of the association, or correlation, as well as the direction of the relationship.\n",
    "\n",
    "A Pearson correlation is a number between -1 and 1.\n",
    "Features highly correlated (close to 1 or -1) with our target 'quality' will be helpful for the predictions of the latter.\n",
    "If a feature has a correlation value close to 0, we can assume that they will be less useful regarding predictions of our target.\n",
    "\n",
    "Let's analyze features correlation with our target to spot which features seems useful for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127    0.337540\n",
      "176    0.217100\n",
      "18     0.206452\n",
      "59     0.203166\n",
      "135    0.179960\n",
      "         ...   \n",
      "108    0.001018\n",
      "111    0.000971\n",
      "246    0.000778\n",
      "263    0.000624\n",
      "269    0.000211\n",
      "Name: target, Length: 301, dtype: float64 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "correlations = training_set.corr()['target'].drop('target').abs().sort_values(ascending=False)\n",
    "print(correlations,'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
